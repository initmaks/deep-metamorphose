{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/reiinakano/neural-painters/blob/master/notebooks/recreating_spiral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqI4x4o6SuSt"
   },
   "source": [
    "# Optionally connect to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9P5f_uzAq_5g"
   },
   "source": [
    "# Install MyPaint and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "nx2C15rB4Huz",
    "outputId": "d8849eb1-e10a-46ad-c1da-0dd93247ae3d"
   },
   "outputs": [],
   "source": [
    "# !pip install future-fstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "1ueV31ATgbQp",
    "outputId": "520338c4-d6c7-4a83-a608-7fbdb1caa0ee"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/reiinakano/SPIRAL-tensorflow.git\n",
    "# !cd SPIRAL-tensorflow && git checkout reiinakano-patch-2  #reiinakano-patches\n",
    "# !cd SPIRAL-tensorflow && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "a2NYZTrugOeu",
    "outputId": "322a159d-b86a-4c29-85f5-44d3459fffea"
   },
   "outputs": [],
   "source": [
    "# ! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# ! unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQA6CkVzTAu4"
   },
   "source": [
    "# Download the CelebA dataset\n",
    "\n",
    "We download the CelebA dataset from Kaggle. You will need to set your Kaggle API credentials via https://github.com/Kaggle/kaggle-api#api-credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "KmSBAF-vTMDU",
    "outputId": "923c58fd-1517-46ab-e806-fe62a630ef4b"
   },
   "outputs": [],
   "source": [
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kwi62fJTNHo"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ~/.kaggle\n",
    "\n",
    "# Make sure to upload your Kaggle key from somewhere. I saved mine in Drive\n",
    "#!cp \"/drive/My Drive/kaggle.json\" ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_QInVqJW0Nwr",
    "outputId": "4b270356-67ed-4f05-88e7-f164631d941e"
   },
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d jessicali9530/celeba-dataset -f img_align_celeba.zip \n",
    "# !unzip -q img_align_celeba.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZNVvr-OXNz8"
   },
   "source": [
    "# Download pre-trained VAE and GAN Neural Painters\n",
    "\n",
    "I have prepared pre-trained VAE and GAN Neural Painters here but feel free to use your own if you wish.\n",
    "\n",
    "Place the VAE in tf_vae and the GAN in tf_gan3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bgx8N4o6uXJE"
   },
   "outputs": [],
   "source": [
    "# !mkdir tf_vae\n",
    "# !wget -O tf_vae/vae-300000.index 'https://docs.google.com/uc?export=download&id=1ulHdDxebH46m_0ZoLa2Wsz_6vStYqJQm'\n",
    "# !wget -O tf_vae/vae-300000.meta 'https://docs.google.com/uc?export=download&id=1nHN_i7Ro9g0lP4y_YQCvIWrOVX1I3CJa'\n",
    "# !wget -O tf_vae/vae-300000.data-00000-of-00001 'https://docs.google.com/uc?export=download&id=18rAJcUJwFJOAcjzsabtqK12udsHMZkVk'\n",
    "# !wget -O tf_vae/checkpoint 'https://docs.google.com/uc?export=download&id=18U4qMNBdyvEk-Y-Mr3MNPEHSHxhcO9hn'\n",
    "\n",
    "# !mkdir tf_gan3\n",
    "# !wget -O tf_gan3/gan-571445.meta 'https://docs.google.com/uc?export=download&id=15kEG1Tiu2FUg5SILVt_9yOsSd3QHwVGA'\n",
    "# !wget -O tf_gan3/gan-571445.index 'https://docs.google.com/uc?export=download&id=11uyFbQsRZoWa9Yq52AFXDXPjPQoGF_ER'\n",
    "# !wget -O tf_gan3/gan-571445.data-00000-of-00001 'https://docs.google.com/uc?export=download&id=11cbvz-CH3KvfZEwNQ2OUujfbf6AKNoQa'\n",
    "# !wget -O tf_gan3/checkpoint 'https://docs.google.com/uc?export=download&id=1A539u51t0L31Ab1M2uPUV2SsCFsNDQRo'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf8wNMfICWqW"
   },
   "source": [
    "# Imports\n",
    "\n",
    "If you encounter `SyntaxError: encoding problem: future_fstrings` then please \"Restart Runtime\" (NOT \"Reset all Runtimes\") and start from this point. I think this is because we are installing a library \"future-fstrings\" but it won't get loaded in until the runtime is reset :( If someone knows a fix please let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "QYBRFuZqRqHL",
    "outputId": "258b3166-f3c5-498a-c968-14c9dc69f879"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.10.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-30355eb5b2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/dl/anaconda3/envs/paint/lib/python3.7/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import sys\n",
    "# sys.path.append('mypaint')\n",
    "sys.path.append('..')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageDraw\n",
    "from collections import defaultdict\n",
    "\n",
    "# from lib import surface, tiledsurface, brush\n",
    "import utils as ut\n",
    "# from envs.mypaint_utils import *\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow.contrib.layers as tcl\n",
    "\n",
    "import imageio\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQmzry2j7SdZ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imageio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4c361698d5b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imageio' is not defined"
     ]
    }
   ],
   "source": [
    "imageio.plugins.freeimage.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQ6yU1vEHZN"
   },
   "outputs": [],
   "source": [
    "# class args:\n",
    "#   jump=True\n",
    "#   curve=True\n",
    "#   screen_size=64\n",
    "#   location_size=32\n",
    "#   color_channel=3\n",
    "#   brush_path='SPIRAL-tensorflow/assets/brushes/dry_brush.myb'\n",
    "#   train=True\n",
    "#   data_dir=Path('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivFxNXdqC7G9"
   },
   "source": [
    "# paint environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqG26OYpDHXe"
   },
   "outputs": [],
   "source": [
    "# class PaintMode:\n",
    "#   STROKES_ONLY = 0\n",
    "#   JUMP_STROKES = 1\n",
    "#   CONNECTED_STROKES = 2\n",
    "\n",
    "# class ColorEnv():\n",
    "#     head = 0.25\n",
    "#     tail = 0.75\n",
    "    \n",
    "#     # all 0 to 1\n",
    "#     actions_to_idx = {\n",
    "#         'pressure': 0,\n",
    "#         'size': 1,\n",
    "#         'control_x': 2,\n",
    "#         'control_y': 3,\n",
    "#         'end_x': 4,\n",
    "#         'end_y': 5,\n",
    "#         'color_r': 6,\n",
    "#         'color_g': 7,\n",
    "#         'color_b': 8,\n",
    "#         'start_x': 9,\n",
    "#         'start_y': 10,\n",
    "#         'entry_pressure': 11,\n",
    "#     }\n",
    "\n",
    "#     def __init__(self, args, paint_mode=PaintMode.JUMP_STROKES):\n",
    "#         self.args = args\n",
    "#         self.paint_mode = paint_mode\n",
    "\n",
    "#         # screen\n",
    "#         self.screen_size = args.screen_size\n",
    "#         self.height, self.width = self.screen_size, self.screen_size\n",
    "#         self.observation_shape = [\n",
    "#                 self.height, self.width, args.color_channel]\n",
    "\n",
    "#         # location\n",
    "#         self.location_size = args.location_size\n",
    "#         self.location_shape = [self.location_size, self.location_size]\n",
    "        \n",
    "#         self.prev_x, self.prev_y, self.prev_pressure = None, None, None\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def pretty_print_action(ac):\n",
    "#         for k, v in ColorEnv.actions_to_idx.items():\n",
    "#             print(k, ac[v])\n",
    "    \n",
    "#     def random_action(self):\n",
    "#         return np.random.uniform(size=[len(self.actions_to_idx)])\n",
    "      \n",
    "#     def reset(self):\n",
    "#         self.intermediate_images = []\n",
    "#         self.prev_x, self.prev_y, self.prev_pressure = None, None, None\n",
    "\n",
    "#         self.s = tiledsurface.Surface()\n",
    "#         self.s.flood_fill(0, 0, (255, 255, 255), (0, 0, 64, 64), 0, self.s)\n",
    "#         self.s.begin_atomic()\n",
    "\n",
    "#         with open(self.args.brush_path) as fp:\n",
    "#             self.bi = brush.BrushInfo(fp.read())\n",
    "#         self.b = brush.Brush(self.bi)\n",
    "\n",
    "#     def draw(self, ac, s=None, dtime=1):\n",
    "#         # Just added this\n",
    "#         if self.paint_mode == PaintMode.STROKES_ONLY:\n",
    "#           self.s.clear()\n",
    "#           self.s.flood_fill(0, 0, (255, 255, 255), (0, 0, 64, 64), 0, self.s)\n",
    "#           self.s.end_atomic()\n",
    "#           self.s.begin_atomic()\n",
    "        \n",
    "#         if s is None:\n",
    "#             s = self.s\n",
    "\n",
    "#         s_x, s_y = ac[self.actions_to_idx['start_x']]*64, ac[self.actions_to_idx['start_y']]*64  \n",
    "#         e_x, e_y = ac[self.actions_to_idx['end_x']]*64, ac[self.actions_to_idx['end_y']]*64\n",
    "#         c_x, c_y = ac[self.actions_to_idx['control_x']]*64, ac[self.actions_to_idx['control_y']]*64\n",
    "#         color = (\n",
    "#             ac[self.actions_to_idx['color_r']],\n",
    "#             ac[self.actions_to_idx['color_g']],\n",
    "#             ac[self.actions_to_idx['color_b']],\n",
    "#         )\n",
    "#         pressure = ac[self.actions_to_idx['pressure']]*0.8\n",
    "#         entry_pressure = ac[self.actions_to_idx['entry_pressure']]*0.8\n",
    "#         size = ac[self.actions_to_idx['size']] * 2.\n",
    "        \n",
    "#         if self.paint_mode == PaintMode.CONNECTED_STROKES:\n",
    "#             if self.prev_x is not None:\n",
    "#                 s_x, s_y, entry_pressure = self.prev_x, self.prev_y, self.prev_pressure\n",
    "#             self.prev_x, self.prev_y, self.prev_pressure = e_x, e_y, pressure\n",
    "\n",
    "#         self.b.brushinfo.set_color_rgb(color)\n",
    "        \n",
    "#         self.b.brushinfo.set_base_value('radius_logarithmic', size)\n",
    "\n",
    "#         # Move brush to starting point without leaving it on the canvas.\n",
    "#         self._stroke_to(s_x, s_y, 0)\n",
    "\n",
    "#         self._draw(s_x, s_y, e_x, e_y, c_x, c_y, entry_pressure, pressure, size, color, dtime)\n",
    "\n",
    "#     def _draw(self, s_x, s_y, e_x, e_y, c_x, c_y,\n",
    "#               entry_pressure, pressure, size, color, dtime):\n",
    "\n",
    "#         # if straight line or jump\n",
    "#         if pressure == 0:\n",
    "#             self.b.stroke_to(\n",
    "#                     self.s.backend, e_x, e_y, pressure, 0, 0, dtime)\n",
    "#         else:\n",
    "#             self.curve(c_x, c_y, s_x, s_y, e_x, e_y, entry_pressure, pressure)\n",
    "            \n",
    "#         # Relieve brush pressure for next jump\n",
    "#         self._stroke_to(e_x, e_y, 0)\n",
    "\n",
    "#         self.s.end_atomic()\n",
    "#         self.s.begin_atomic()\n",
    "\n",
    "#     # sx, sy = starting point\n",
    "#     # ex, ey = end point\n",
    "#     # kx, ky = curve point from last line\n",
    "#     # lx, ly = last point from InteractionMode update\n",
    "#     def curve(self, cx, cy, sx, sy, ex, ey, entry_pressure, pressure):\n",
    "#         #entry_p, midpoint_p, junk, prange2, head, tail\n",
    "#         entry_p, midpoint_p, prange1, prange2, h, t = \\\n",
    "#                 self._line_settings(entry_pressure, pressure)\n",
    "\n",
    "#         points_in_curve = 100\n",
    "#         mx, my = midpoint(sx, sy, ex, ey)\n",
    "#         length, nx, ny = length_and_normal(mx, my, cx, cy)\n",
    "#         cx, cy = multiply_add(mx, my, nx, ny, length*2)\n",
    "#         x1, y1 = difference(sx, sy, cx, cy)\n",
    "#         x2, y2 = difference(cx, cy, ex, ey)\n",
    "#         head = points_in_curve * h\n",
    "#         head_range = int(head)+1\n",
    "#         tail = points_in_curve * t\n",
    "#         tail_range = int(tail)+1\n",
    "#         tail_length = points_in_curve - tail\n",
    "\n",
    "#         # Beginning\n",
    "#         px, py = point_on_curve_1(1, cx, cy, sx, sy, x1, y1, x2, y2)\n",
    "#         length, nx, ny = length_and_normal(sx, sy, px, py)\n",
    "#         bx, by = multiply_add(sx, sy, nx, ny, 0.25)\n",
    "#         self._stroke_to(bx, by, entry_p)\n",
    "#         pressure = abs(1/head * prange1 + entry_p)\n",
    "#         self._stroke_to(px, py, pressure)\n",
    "\n",
    "#         for i in xrange(2, head_range):\n",
    "#             px, py = point_on_curve_1(i, cx, cy, sx, sy, x1, y1, x2, y2)\n",
    "#             pressure = abs(i/head * prange1 + entry_p)\n",
    "#             self._stroke_to(px, py, pressure)\n",
    "\n",
    "#         # Middle\n",
    "#         for i in xrange(head_range, tail_range):\n",
    "#             px, py = point_on_curve_1(i, cx, cy, sx, sy, x1, y1, x2, y2)\n",
    "#             self._stroke_to(px, py, midpoint_p)\n",
    "\n",
    "#         # End\n",
    "#         for i in xrange(tail_range, points_in_curve+1):\n",
    "#             px, py = point_on_curve_1(i, cx, cy, sx, sy, x1, y1, x2, y2)\n",
    "#             pressure = abs((i-tail)/tail_length * prange2 + midpoint_p)\n",
    "#             self._stroke_to(px, py, pressure)\n",
    "\n",
    "#         return pressure\n",
    "\n",
    "#     def _stroke_to(self, x, y, pressure, duration=0.1):\n",
    "#         self.b.stroke_to(\n",
    "#                 self.s.backend,\n",
    "#                 x, y,\n",
    "#                 pressure,\n",
    "#                 0.0, 0.0,\n",
    "#                 duration)\n",
    "#         self.s.end_atomic()\n",
    "#         self.s.begin_atomic()\n",
    "#         self.intermediate_images.append(self.image)\n",
    "\n",
    "#     def save_image(self, path=\"test.png\"):\n",
    "#         Image.fromarray(self.image.astype(np.uint8).squeeze()).save(path)\n",
    "#         #self.s.save_as_png(path, alpha=False)\n",
    "\n",
    "#     @property\n",
    "#     def image(self):\n",
    "#         rect = [0, 0, self.height, self.width]\n",
    "#         scanline_strips = \\\n",
    "#                 surface.scanline_strips_iter(self.s, rect)\n",
    "#         return next(scanline_strips)\n",
    "\n",
    "#     def _line_settings(self, entry_pressure, pressure):\n",
    "#         p1 = entry_pressure\n",
    "#         p2 = (entry_pressure + pressure) / 2\n",
    "#         p3 = pressure\n",
    "#         if self.head == 0.0001:\n",
    "#             p1 = p2\n",
    "#         prange1 = p2 - p1\n",
    "#         prange2 = p3 - p2\n",
    "#         return p1, p2, prange1, prange2, self.head, self.tail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2sqbc-4D_pk"
   },
   "source": [
    "# celeba environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXATLvvnmGcW"
   },
   "outputs": [],
   "source": [
    "class CelebADispenser():\n",
    "\n",
    "    def __init__(self, data_dir=Path('data'), screen_size=64):\n",
    "        self.data_dir = data_dir\n",
    "        self.height, self.width = screen_size, screen_size\n",
    "        self.prepare_data()\n",
    "\n",
    "    def get_random_target(self, num=1, squeeze=False, train=True):\n",
    "        data_length = self.real_data.shape[0]\n",
    "        train_len = int(data_length*0.9)\n",
    "        if train:\n",
    "          a = train_len\n",
    "        else:\n",
    "          a = np.arange(train_len, data_length)\n",
    "        random_idxes = np.random.choice(a, num, replace=False)\n",
    "        random_image = self.real_data[random_idxes]\n",
    "        if squeeze:\n",
    "            random_image = np.squeeze(random_image, 0)\n",
    "        return random_image\n",
    "      \n",
    "    def prepare_data(self):\n",
    "        ut.io.makedirs(self.data_dir)\n",
    "        \n",
    "        omniglot_image_files = tf.gfile.Glob('img_align_celeba/*')\n",
    "\n",
    "        # ground truth omniglot data\n",
    "        mnist_dir = self.data_dir / 'celeba'\n",
    "        try:\n",
    "          os.makedirs(str(mnist_dir))\n",
    "        except OSError:\n",
    "          pass\n",
    "        pkl_path = mnist_dir / 'celeba_dict.pkl'\n",
    "        if pkl_path.exists():\n",
    "            self.real_data = np.load(pkl_path)\n",
    "        else:\n",
    "            omniglot_list = []\n",
    "            #iterator = tqdm(omniglot_image_files, desc=\"Processing\")\n",
    "            for idx, img in enumerate(omniglot_image_files):\n",
    "              if idx % 10000 == 0: print(idx)\n",
    "              img = ut.io.imread(img)\n",
    "              img = ut.io.imresize(img, (self.height, self.width))\n",
    "#                                 interpolation='cubic') #TODO\n",
    "              omniglot_list.append(img)\n",
    "                          \n",
    "            self.real_data = np.concatenate(omniglot_list).reshape(-1, 64, 64, 3)\n",
    "            with open(str(pkl_path), 'wb') as f:\n",
    "              np.save(f, self.real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLMmcTHo7YT4"
   },
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "#celeba_dispenser = CelebADispenser()\n",
    "#celeba_dispenser.get_random_target().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6I5xeZ6GUC-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSF1EvQvGUUj"
   },
   "source": [
    "# VAE Painter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axiRNmrtlcAK"
   },
   "outputs": [],
   "source": [
    "# class ConvVAE(object):\n",
    "#   def __init__(self, z_size=64, batch_size=100, learning_rate=0.0001, kl_tolerance=0.5, is_training=True, reuse=False, gpu_mode=True, graph=None):\n",
    "#     self.z_size = z_size\n",
    "#     self.batch_size = batch_size\n",
    "#     self.learning_rate = learning_rate\n",
    "#     self.is_training = is_training\n",
    "#     self.kl_tolerance = kl_tolerance\n",
    "#     self.reuse = reuse\n",
    "#     # Is it okay to comment this out? with tf.variable_scope('conv_vae', reuse=self.reuse):\n",
    "#     if not gpu_mode:\n",
    "#       with tf.device('/cpu:0'):\n",
    "#         tf.logging.info('conv_vae using cpu.')\n",
    "#         self._build_graph(graph)\n",
    "#     else:\n",
    "#       tf.logging.info('conv_vae using gpu.')\n",
    "#       self._build_graph(graph)\n",
    "#     self._init_session()\n",
    "  \n",
    "#   def build_decoder(self, z, reuse=False):\n",
    "#     with tf.variable_scope('decoder', reuse=reuse):\n",
    "#       h = tf.layers.dense(z, 4*256, name=\"fc\")\n",
    "#       h = tf.reshape(h, [-1, 1, 1, 4*256])\n",
    "#       h = tf.layers.conv2d_transpose(h, 128, 5, strides=2, activation=tf.nn.relu, name=\"deconv1\")\n",
    "#       h = tf.layers.conv2d_transpose(h, 64, 5, strides=2, activation=tf.nn.relu, name=\"deconv2\")\n",
    "#       h = tf.layers.conv2d_transpose(h, 32, 6, strides=2, activation=tf.nn.relu, name=\"deconv3\")\n",
    "#       return tf.layers.conv2d_transpose(h, 3, 6, strides=2, activation=tf.nn.sigmoid, name=\"deconv4\")\n",
    "  \n",
    "#   def build_predictor(self, actions, reuse=False, is_training=False):\n",
    "#     with tf.variable_scope('predictor', reuse=reuse):\n",
    "#       h = tf.layers.dense(actions, 256, activation=tf.nn.leaky_relu, name=\"fc1\")\n",
    "#       h = tf.layers.batch_normalization(h, training=is_training, name=\"bn1\")\n",
    "#       h = tf.layers.dense(h, 64, activation=tf.nn.leaky_relu, name=\"fc2\")\n",
    "#       h = tf.layers.batch_normalization(h, training=is_training, name=\"bn2\")\n",
    "#       h = tf.layers.dense(h, 64, activation=tf.nn.leaky_relu, name=\"fc3\")\n",
    "#       h = tf.layers.batch_normalization(h, training=is_training, name=\"bn3\")\n",
    "#       return tf.layers.dense(h, self.z_size, name='fc4')\n",
    "  \n",
    "#   def _build_graph(self, graph):\n",
    "#     if graph is None:\n",
    "#       self.g = tf.Graph()\n",
    "#     else:\n",
    "#       self.g = graph\n",
    "#     with self.g.as_default(), tf.variable_scope('conv_vae', reuse=self.reuse):\n",
    "\n",
    "#       #### autoencoding part\n",
    "#       self.x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n",
    "      \n",
    "#       # Encoder\n",
    "#       h = tf.layers.conv2d(self.x, 32, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv1\")\n",
    "#       h = tf.layers.conv2d(h, 64, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv2\")\n",
    "#       h = tf.layers.conv2d(h, 128, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv3\")\n",
    "#       h = tf.layers.conv2d(h, 256, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv4\")\n",
    "#       h = tf.reshape(h, [-1, 2*2*256])\n",
    "\n",
    "#       # VAE\n",
    "#       self.mu = tf.layers.dense(h, self.z_size, name=\"enc_fc_mu\")\n",
    "#       self.logvar = tf.layers.dense(h, self.z_size, name=\"enc_fc_log_var\")\n",
    "#       self.sigma = tf.exp(self.logvar / 2.0)\n",
    "#       #self.epsilon = tf.random_normal([self.batch_size, self.z_size])\n",
    "#       self.epsilon = tf.random_normal([tf.shape(self.sigma)[0], self.z_size])\n",
    "#       self.z = self.mu + self.sigma * self.epsilon\n",
    "\n",
    "#       # Decoder\n",
    "#       self.y = self.build_decoder(self.z)\n",
    "      \n",
    "#       #### predicting part\n",
    "#       self.actions = tf.placeholder(tf.float32, shape=[None, 12])\n",
    "#       self.predicted_z = self.build_predictor(self.actions, is_training=self.is_training)\n",
    "#       self.predicted_y = self.build_decoder(self.predicted_z, reuse=True)\n",
    "      \n",
    "#       # train ops\n",
    "#       if self.is_training:\n",
    "#         self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "#         self.uneven_multiplier = tf.placeholder_with_default(1.0, [])\n",
    "#         summ_uneven_mult = tf.summary.scalar('uneven_multiplier', self.uneven_multiplier)\n",
    "\n",
    "#         eps = 1e-6 # avoid taking log of zero\n",
    "        \n",
    "#         mask = tf.reduce_mean(\n",
    "#           self.x,\n",
    "#           reduction_indices = [3]\n",
    "#         )\n",
    "#         stroke_whitespace = tf.equal(mask, 1.0)\n",
    "#         mask = tf.where(stroke_whitespace, tf.ones(tf.shape(mask)), self.uneven_multiplier*tf.ones(tf.shape(mask)))\n",
    "#         mask = tf.reshape(mask, [-1, 64, 64, 1])\n",
    "#         mask = tf.tile(mask, [1, 1, 1, 3])\n",
    "#         summ_mask = tf.summary.image('mask', mask, max_outputs=3)\n",
    "#         print(mask.get_shape())\n",
    "        \n",
    "#         # reconstruction loss\n",
    "#         self.r_loss = tf.reduce_sum(\n",
    "#           (tf.square(self.x - self.y))*mask,\n",
    "#           reduction_indices = [1,2,3]\n",
    "#         )\n",
    "#         self.r_loss = tf.reduce_mean(self.r_loss)\n",
    "\n",
    "#         # augmented kl loss per dim\n",
    "#         self.kl_loss = - 0.5 * tf.reduce_sum(\n",
    "#           (1 + self.logvar - tf.square(self.mu) - tf.exp(self.logvar)),\n",
    "#           reduction_indices = 1\n",
    "#         )\n",
    "#         self.kl_loss = tf.maximum(self.kl_loss, self.kl_tolerance * self.z_size)\n",
    "#         self.kl_loss = tf.reduce_mean(self.kl_loss)\n",
    "        \n",
    "#         summ_recon_loss = tf.summary.scalar('recon_loss', self.r_loss)\n",
    "#         summ_kl_loss = tf.summary.scalar('kl_loss', self.kl_loss)\n",
    "#         self.loss = self.r_loss + self.kl_loss\n",
    "        \n",
    "#         # training the vae\n",
    "#         self.lr = tf.Variable(self.learning_rate, trainable=False)\n",
    "#         summ_lr = tf.summary.scalar('lr', self.lr)\n",
    "#         self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "#         grads = self.optimizer.compute_gradients(\n",
    "#             self.loss, \n",
    "#             tf.global_variables('conv_vae/enc_*')+tf.global_variables('conv_vae/decoder*')) # can potentially clip gradients here.\n",
    "\n",
    "#         self.train_op = self.optimizer.apply_gradients(\n",
    "#           grads, global_step=self.global_step, name='train_step')\n",
    "#         summ_loss = tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "#         # training the predictor\n",
    "#         self.predictor_loss = tf.reduce_mean(tf.square(self.predicted_z - self.z))\n",
    "#         self.optimizer2 = tf.train.AdamOptimizer(self.lr)\n",
    "#         grads2 = self.optimizer2.compute_gradients(self.predictor_loss, tf.global_variables('conv_vae/predictor*'))\n",
    "#         with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "#           self.train_op2 = self.optimizer2.apply_gradients(\n",
    "#             grads2, global_step=self.global_step, name='train_step2')\n",
    "#         summ_predictor_loss = tf.summary.scalar('predictor_loss', self.predictor_loss)\n",
    "      \n",
    "#         #summary ops\n",
    "#         summ_inp_img = tf.summary.image('inp_img', self.x, max_outputs=3)\n",
    "#         summ_output_img = tf.summary.image('output_img', self.y, max_outputs=3)\n",
    "#         summ_predicted_output_img = tf.summary.image('predicted_output_img', self.predicted_y, max_outputs=3)\n",
    "#         self.summary_op = tf.summary.merge([\n",
    "#             summ_inp_img, summ_output_img, summ_loss, summ_kl_loss, summ_recon_loss,\n",
    "#             summ_mask, summ_uneven_mult, summ_lr\n",
    "#         ])\n",
    "#         self.summary_op_2 = tf.summary.merge([\n",
    "#             summ_inp_img, summ_predictor_loss, summ_output_img, summ_predicted_output_img, summ_lr\n",
    "#         ])\n",
    "        \n",
    "\n",
    "#       # initialize vars\n",
    "#       self.init = tf.global_variables_initializer()\n",
    "  \n",
    "#   def generate_stroke_graph(self, actions):\n",
    "#     with tf.variable_scope('conv_vae', reuse=True):\n",
    "#       with self.g.as_default():\n",
    "#         # Encoder?\n",
    "#         z = self.build_predictor(actions, reuse=True, is_training=False)\n",
    "\n",
    "#         # Decoder\n",
    "#         return self.build_decoder(z, reuse=True)\n",
    "\n",
    "#   def _init_session(self):\n",
    "#     \"\"\"Launch TensorFlow session and initialize variables\"\"\"\n",
    "#     self.sess = tf.Session(graph=self.g)\n",
    "#     self.sess.run(self.init)\n",
    "#   def close_sess(self):\n",
    "#     \"\"\" Close TensorFlow session \"\"\"\n",
    "#     self.sess.close()\n",
    "#   def save_model(self, model_save_path='tf_vae'):\n",
    "#     sess = self.sess\n",
    "#     step = sess.run(self.global_step)\n",
    "#     with self.g.as_default():\n",
    "#       saver = tf.train.Saver(tf.global_variables())\n",
    "#     checkpoint_path = os.path.join(model_save_path, 'vae')\n",
    "#     tf.logging.info('saving model %s.', checkpoint_path)\n",
    "#     saver.save(sess, checkpoint_path, step) # just keep one\n",
    "#   def load_checkpoint(self, checkpoint_path='tf_vae', actual_path=None):\n",
    "#     sess = self.sess\n",
    "#     with self.g.as_default():\n",
    "#       saver = tf.train.Saver(tf.global_variables())\n",
    "#     ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "#     if actual_path is None:\n",
    "#       actual_path = ckpt.model_checkpoint_path\n",
    "#     print('loading model', actual_path)\n",
    "#     tf.logging.info('Loading model %s.', actual_path)\n",
    "#     saver.restore(sess, actual_path)\n",
    "#   def load_only_vae_checkpoint(self, checkpoint_path='tf_vae', actual_path=None):\n",
    "#     sess = self.sess\n",
    "#     with self.g.as_default():\n",
    "#       to_save = tf.global_variables('conv_vae/enc_*')+tf.global_variables('conv_vae/decoder*')+tf.global_variables('conv_vae/conv_vae/enc_*')+tf.global_variables('conv_vae/conv_vae/decoder*')+tf.global_variables('conv_vae/global_step')\n",
    "#       print(to_save)\n",
    "#       saver = tf.train.Saver(to_save)\n",
    "#     ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "#     if actual_path is None:\n",
    "#       actual_path = ckpt.model_checkpoint_path\n",
    "#     print('loading model', actual_path)\n",
    "#     tf.logging.info('Loading model %s.', actual_path)\n",
    "#     saver.restore(sess, actual_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfMJ6xCEoVSu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-q7nTIXRmFBJ"
   },
   "source": [
    "# GAN Painter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qS1xkGT3mGDm"
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.2):\n",
    "    return tf.maximum(tf.minimum(0.0, alpha * x), x)\n",
    "\n",
    "def leaky_relu_batch_norm(x, alpha=0.2):\n",
    "    return leaky_relu(tf.contrib.layers.batch_norm(x, updates_collections=None), alpha)\n",
    "\n",
    "def relu_batch_norm(x):\n",
    "    return tf.nn.relu(tf.contrib.layers.batch_norm(x, updates_collections=None))\n",
    "  \n",
    "class DiscriminatorConditionalWM(object):\n",
    "    def __init__(self, divisor=1):\n",
    "        \"\"\"\n",
    "        make the network smaller by divisor times\n",
    "        \"\"\"\n",
    "        self.x_dim = 64 * 64 * 3\n",
    "        self.name = 'lsun/dcgan/d_net'\n",
    "        self.divisor=divisor\n",
    "        \n",
    "    def __call__(self, x, conditions, reuse=True):\n",
    "        with tf.variable_scope(self.name) as vs:\n",
    "            if reuse:\n",
    "                vs.reuse_variables()\n",
    "            bs = tf.shape(x)[0]\n",
    "            x = tf.reshape(x, [bs, 64, 64, 3])\n",
    "            conditions = tf.reshape(conditions, [bs, 2])\n",
    "            conditions = tf.contrib.layers.fully_connected(conditions, 64//self.divisor)\n",
    "            conditions = tf.reshape(conditions, [bs, 1, 1, 64//self.divisor])\n",
    "            conv1 = tcl.conv2d(\n",
    "                x, 64//self.divisor, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu\n",
    "            )\n",
    "            conv1 = conv1 + conditions\n",
    "            conv2 = tcl.conv2d(\n",
    "                conv1, 128//self.divisor, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu_batch_norm\n",
    "            )\n",
    "            conv3 = tcl.conv2d(\n",
    "                conv2, 256//self.divisor, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu_batch_norm\n",
    "            )\n",
    "            conv4 = tcl.conv2d(\n",
    "                conv3, 512//self.divisor, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu_batch_norm\n",
    "            )\n",
    "            conv4 = tcl.flatten(conv4)\n",
    "            fc = tcl.fully_connected(conv4, 1, activation_fn=tf.identity)\n",
    "            return fc\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return [var for var in tf.global_variables() if self.name in var.name]\n",
    "\n",
    "class GeneratorConditional(object):\n",
    "    def __init__(self, divisor=1, add_noise=False):\n",
    "        self.x_dim = 64 * 64 * 3\n",
    "        self.divisor=divisor\n",
    "        self.name = 'lsun/dcgan/g_net'\n",
    "        self.add_noise = add_noise\n",
    "\n",
    "    def __call__(self, conditions, is_training):\n",
    "        with tf.contrib.framework.arg_scope([tcl.batch_norm], \n",
    "                                            is_training=is_training):\n",
    "          with tf.variable_scope(self.name) as vs:\n",
    "              bs = tf.shape(conditions)[0]\n",
    "              if self.add_noise:\n",
    "                conditions = tf.concat([conditions, tf.random.uniform([bs, 10])], axis=1)\n",
    "              fc = tcl.fully_connected(conditions, 4 * 4 * 1024//self.divisor, activation_fn=tf.identity)\n",
    "              conv1 = tf.reshape(fc, tf.stack([bs, 4, 4, 1024//self.divisor]))\n",
    "              conv1 = relu_batch_norm(conv1)\n",
    "              conv2 = tcl.conv2d_transpose(\n",
    "                  conv1, 512//self.divisor, [4, 4], [2, 2],\n",
    "                  weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                  activation_fn=relu_batch_norm\n",
    "              )\n",
    "              conv3 = tcl.conv2d_transpose(\n",
    "                  conv2, 256//self.divisor, [4, 4], [2, 2],\n",
    "                  weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                  activation_fn=relu_batch_norm\n",
    "              )\n",
    "              conv4 = tcl.conv2d_transpose(\n",
    "                  conv3, 128//self.divisor, [4, 4], [2, 2],\n",
    "                  weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                  activation_fn=relu_batch_norm\n",
    "              )\n",
    "              conv5 = tcl.conv2d_transpose(\n",
    "                  conv4, 3, [4, 4], [2, 2],\n",
    "                  weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                  activation_fn=tf.sigmoid)\n",
    "              return conv5\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return [var for var in tf.global_variables() if self.name in var.name]\n",
    "      \n",
    "class ConvGAN(object):\n",
    "  def __init__(self, learning_rate=0.0001, is_training=True, reuse=False, gpu_mode=True, data_glob='data/episodes_*.npz', divisor=1, add_noise=False, graph=None):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.is_training = is_training\n",
    "    self.reuse = reuse\n",
    "    self.g_net = GeneratorConditional(divisor=divisor, add_noise=add_noise)\n",
    "    self.d_net = DiscriminatorConditionalWM(divisor=divisor)\n",
    "    \n",
    "    if self.is_training:\n",
    "      self.episode_files = tf.gfile.Glob(data_glob)\n",
    "      np.random.shuffle(self.episode_files)\n",
    "      self.ep_file_ctr = 0\n",
    "      self.current_file_idx = 0\n",
    "      loaded = np.load(self.episode_files[0])\n",
    "      self.loaded_strokes = loaded['strokes']\n",
    "      self.loaded_actions = loaded['actions']\n",
    "      self.len_loaded = len(self.loaded_strokes)\n",
    "    \n",
    "    if not gpu_mode:\n",
    "      with tf.device('/cpu:0'):\n",
    "        tf.logging.info('conv_gan using cpu.')\n",
    "        self._build_training_or_generator_graph(graph)\n",
    "    else:\n",
    "      tf.logging.info('conv_gan using gpu.')\n",
    "      self._build_training_or_generator_graph(graph)\n",
    "    self._init_session()\n",
    "   \n",
    "  def _build_training_or_generator_graph(self, graph):\n",
    "    if self.is_training:\n",
    "      self._build_graph(graph)\n",
    "    else:\n",
    "      self._build_generator_graph(graph)\n",
    "  \n",
    "  def _build_graph(self, graph):\n",
    "    if graph is None:\n",
    "      self.g = tf.Graph()\n",
    "    else:\n",
    "      self.g = graph\n",
    "    with self.g.as_default(), tf.variable_scope('conv_gan', reuse=self.reuse):\n",
    "\n",
    "      self.x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n",
    "      self.actions = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "      \n",
    "      self.y = self.g_net(self.actions, is_training=self.is_training)\n",
    "      \n",
    "      real_score = tf.reduce_mean(self.d_net(self.x, self.actions, reuse=False))\n",
    "      generated_score = tf.reduce_mean(self.d_net(self.y, self.actions))\n",
    "      \n",
    "      real_score_summ = tf.summary.scalar('real_score', real_score)\n",
    "      g_loss_summ = tf.summary.scalar('generated_score/g_loss', generated_score)\n",
    "      \n",
    "      reconstruction_loss = tf.reduce_mean(tf.square(self.y-self.x))\n",
    "      recon_loss_summ = tf.summary.scalar('recon_loss', reconstruction_loss)\n",
    "      \n",
    "      # actual reconstruction loss used for training\n",
    "      self.uneven_multiplier = tf.placeholder_with_default(1.0, [])\n",
    "      u_m_summ = tf.summary.scalar('uneven_multiplier', self.uneven_multiplier)\n",
    "      mask = tf.reduce_mean(\n",
    "        self.x,\n",
    "        reduction_indices = [3]\n",
    "      )\n",
    "      stroke_whitespace = tf.equal(mask, 1.0)\n",
    "      mask = tf.where(stroke_whitespace, tf.ones(tf.shape(mask)), self.uneven_multiplier*tf.ones(tf.shape(mask)))\n",
    "      mask = tf.reshape(mask, [-1, 64, 64, 1])\n",
    "      mask = tf.tile(mask, [1, 1, 1, 3])\n",
    "      tf.summary.image('mask', mask, max_outputs=3)\n",
    "      print(mask.get_shape())\n",
    "      self.r_loss = 10*tf.reduce_mean((tf.square(self.x - self.y))*mask)\n",
    "      actual_recon_loss_summ = tf.summary.scalar('recon_loss_actual', self.r_loss)\n",
    "      # /actual reconstruction loss used for training\n",
    "      \n",
    "      self.g_loss = generated_score + self.r_loss\n",
    "      self.d_loss = real_score - generated_score\n",
    "      \n",
    "      epsilon = tf.random_uniform([], 0.0, 1.0)\n",
    "      x_hat = epsilon * self.x + (1 - epsilon) * self.y\n",
    "      d_hat = self.d_net(x_hat, self.actions)\n",
    "      \n",
    "      ddx = tf.gradients(d_hat, x_hat)[0]\n",
    "      print(ddx.get_shape().as_list())\n",
    "\n",
    "      ddx = tf.sqrt(tf.reduce_sum(tf.square(ddx), axis=(1, 2, 3)))\n",
    "      ddx = tf.reduce_mean(tf.square(ddx - 1.0) * 10.0)\n",
    "      \n",
    "      self.d_loss = self.d_loss + ddx\n",
    "      \n",
    "      gradient_penalty_summ = tf.summary.scalar('gradient_penalty', ddx)\n",
    "      d_loss_summ = tf.summary.scalar('actual_loss', self.d_loss)\n",
    "      \n",
    "      # train ops\n",
    "      if self.is_training:\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.inc_global_step = tf.assign(self.global_step, self.global_step+1)\n",
    "\n",
    "        self.d_adam, self.g_adam = None, None\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.d_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9)\\\n",
    "                .minimize(self.d_loss, var_list=self.d_net.vars)\n",
    "            self.g_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9)\\\n",
    "                .minimize(self.g_loss, var_list=self.g_net.vars)\n",
    "\n",
    "      # initialize vars\n",
    "      self.init = tf.global_variables_initializer()\n",
    "      \n",
    "      #summary ops\n",
    "      tf.summary.image('inp_img', self.x, max_outputs=3)\n",
    "      tf.summary.image('output_img', self.y, max_outputs=3)\n",
    "      self.summary_op = tf.summary.merge_all()\n",
    "      \n",
    "  def _build_generator_graph(self, graph):\n",
    "    if graph is None:\n",
    "      self.g = tf.Graph()\n",
    "    else:\n",
    "      self.g = graph\n",
    "      \n",
    "    with self.g.as_default(), tf.variable_scope('conv_gan', reuse=self.reuse):\n",
    "      self.actions = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "      self.y = self.g_net(self.actions, is_training=self.is_training)\n",
    "      self.init = tf.global_variables_initializer()\n",
    "  \n",
    "  def generate_stroke_graph(self, actions):\n",
    "    with tf.variable_scope('conv_gan', reuse=True):\n",
    "      with self.g.as_default():\n",
    "        return self.g_net(actions, is_training=False)\n",
    "      \n",
    "  def get_random_batch(self, batch_size):\n",
    "    while self.len_loaded - self.current_file_idx < batch_size:\n",
    "      self.ep_file_ctr = (self.ep_file_ctr + 1) % len(self.episode_files)\n",
    "      self.current_file_idx = 0\n",
    "      print('loading new file', self.episode_files[self.ep_file_ctr])\n",
    "      loaded = np.load(self.episode_files[self.ep_file_ctr])\n",
    "      self.loaded_strokes = loaded['strokes']\n",
    "      self.loaded_actions = loaded['actions']\n",
    "      self.len_loaded = len(self.loaded_strokes)\n",
    "      rand_idx = np.random.permutation(self.len_loaded)\n",
    "      self.loaded_strokes = self.loaded_strokes[rand_idx]\n",
    "      self.loaded_actions = self.loaded_actions[rand_idx]\n",
    "    \n",
    "    strokes = self.loaded_strokes[self.current_file_idx:self.current_file_idx+batch_size]\n",
    "    strokes = strokes.astype(np.float)/255.0\n",
    "    actions = self.loaded_actions[self.current_file_idx:self.current_file_idx+batch_size]\n",
    "    \n",
    "    self.current_file_idx += batch_size\n",
    "    \n",
    "    return strokes, actions\n",
    "      \n",
    "  def train(self, batch_size=64, num_batches=1000000):\n",
    "    train_writer = tf.summary.FileWriter('logdir', self.g)\n",
    "    start_time = time.time()\n",
    "    uneven_multiplier = 10.\n",
    "    for t in range(num_batches):\n",
    "        d_iters = 5\n",
    "\n",
    "        for _ in range(0, d_iters):\n",
    "            strokes, actions = self.get_random_batch(batch_size)\n",
    "            _, summ = self.sess.run((self.d_adam, self.summary_op), \n",
    "                                    feed_dict={self.x: strokes, self.actions: actions})\n",
    "\n",
    "        strokes, actions = self.get_random_batch(batch_size)\n",
    "        _, summ, _, step = self.sess.run((self.g_adam, self.summary_op, self.inc_global_step, self.global_step), \n",
    "                                      feed_dict={self.x: strokes, \n",
    "                                                 self.actions: actions, \n",
    "                                                 self.uneven_multiplier: uneven_multiplier})\n",
    "        train_writer.add_summary(summ, step)\n",
    "        \n",
    "        if step > 295044:\n",
    "          uneven_multiplier = 1.\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            strokes, actions = self.get_random_batch(batch_size)\n",
    "            d_loss = self.sess.run(\n",
    "                self.d_loss, feed_dict={self.x: strokes, self.actions: actions}\n",
    "            )\n",
    "            g_loss = self.sess.run(\n",
    "                self.g_loss, feed_dict={self.x: strokes, self.actions: actions}\n",
    "            )\n",
    "            print('Iter [%8d] Time [%5.4f] d_loss [%.4f] g_loss [%.4f]' %\n",
    "                    (step, time.time() - start_time, d_loss, g_loss))\n",
    "            \n",
    "        if step % 2000 == 0:\n",
    "            self.save_model()\n",
    "\n",
    "  def _init_session(self):\n",
    "    \"\"\"Launch TensorFlow session and initialize variables\"\"\"\n",
    "    self.sess = tf.Session(graph=self.g)\n",
    "    self.sess.run(self.init)\n",
    "  def close_sess(self):\n",
    "    \"\"\" Close TensorFlow session \"\"\"\n",
    "    self.sess.close()\n",
    "  def save_model(self, model_save_path='tf_gan3'):\n",
    "    sess = self.sess\n",
    "    step = sess.run(self.global_step)\n",
    "    with self.g.as_default():\n",
    "      saver = tf.train.Saver(tf.global_variables())\n",
    "    checkpoint_path = os.path.join(model_save_path, 'gan')\n",
    "    tf.logging.info('saving model %s.', checkpoint_path)\n",
    "    saver.save(sess, checkpoint_path, step) # just keep one\n",
    "  def load_checkpoint(self, checkpoint_path='tf_gan3', actual_path=None):\n",
    "    sess = self.sess\n",
    "    with self.g.as_default():\n",
    "      saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "    if actual_path is None:\n",
    "      actual_path = ckpt.model_checkpoint_path\n",
    "    print('loading model', actual_path)\n",
    "    tf.logging.info('Loading model %s.', actual_path)\n",
    "    saver.restore(sess, actual_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gm--vvK2HC1P"
   },
   "source": [
    "# Adversarial Training graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZF5_lQat8oC"
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.2):\n",
    "    return tf.maximum(tf.minimum(0.0, alpha * x), x)\n",
    "\n",
    "def leaky_relu_batch_norm(x, alpha=0.2):\n",
    "    return leaky_relu(tf.contrib.layers.batch_norm(x, updates_collections=None), alpha)\n",
    "\n",
    "def relu_batch_norm(x):\n",
    "    return tf.nn.relu(tf.contrib.layers.batch_norm(x, updates_collections=None))\n",
    "\n",
    "def res_block(x, channel, size, name):\n",
    "    with tf.variable_scope(name):\n",
    "        enc_x = tf.contrib.layers.conv2d(\n",
    "                x, channel, size,\n",
    "                padding='same',\n",
    "                activation_fn=tf.nn.relu)\n",
    "\n",
    "        res = tf.contrib.layers.conv2d(\n",
    "                enc_x, channel, size,\n",
    "                padding='same',\n",
    "                activation_fn=None) + x\n",
    "    return res\n",
    "\n",
    "class ActionGenerator2(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'action_generator2'\n",
    "        self.cell = None\n",
    "        \n",
    "    def create_cell(self):\n",
    "        with tf.variable_scope(self.name) as vs:\n",
    "            self.cell = tf.nn.rnn_cell.LSTMCell(256)\n",
    "            return self.cell\n",
    "\n",
    "    def __call__(self, prev_cell_state, prev_hidden_state, prev_actions, current_canvas, target_image, reuse=True):\n",
    "        if self.cell is None:\n",
    "            self.create_cell()\n",
    "            \n",
    "        with tf.variable_scope(self.name) as vs:\n",
    "            if reuse:\n",
    "                vs.reuse_variables()\n",
    "            bs = tf.shape(prev_hidden_state)[0]\n",
    "            \n",
    "            prev_actions = tf.contrib.layers.fully_connected(prev_actions, 32)\n",
    "            concat_actions_world = tf.reshape(prev_actions, [bs, 1, 1, 32])\n",
    "            \n",
    "            current_canvas = tf.reshape(current_canvas, [bs, 64, 64, 3])\n",
    "            target_image = tf.reshape(target_image, [bs, 64, 64, 3])\n",
    "            concat_canvas = tf.concat([current_canvas, target_image], axis=3)  # Does this make sense? idk\n",
    "            \n",
    "            concat_canvas = tf.contrib.layers.conv2d(\n",
    "              concat_canvas, 32, 5, scope=\"x_c_enc\"\n",
    "            )\n",
    "            \n",
    "            conditioned_canvas = concat_canvas + concat_actions_world\n",
    "            \n",
    "            for idx in range(int(3)):\n",
    "                conditioned_canvas = tf.contrib.layers.conv2d(\n",
    "                        conditioned_canvas, 32, 4, stride=(2, 2),\n",
    "                        padding='valid',\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        scope=\"add_enc_{}\".format(idx))\n",
    "            for idx in range(8):\n",
    "                conditioned_canvas = res_block(\n",
    "                        conditioned_canvas, 32, 3,\n",
    "                        name=\"encoder_res_{}\".format(idx))\n",
    "                \n",
    "            conditioned_canvas = tf.contrib.layers.flatten(conditioned_canvas)\n",
    "            \n",
    "            lstm_in = tf.contrib.layers.fully_connected(conditioned_canvas, 256)\n",
    "            \n",
    "            lstm_out, lstm_state = self.cell(lstm_in, tf.nn.rnn_cell.LSTMStateTuple(prev_cell_state, prev_hidden_state))\n",
    "            \n",
    "            fc = tf.contrib.layers.fully_connected(lstm_out, 2, activation_fn=tf.sigmoid)\n",
    "            #fc = tf.concat([tf.zeros([bs, 1]), fc[:, 1:]], axis=1)\n",
    "            return fc, lstm_state\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return [var for var in tf.global_variables() if self.name in var.name]\n",
    "\n",
    "class DiscriminatorConditional(object):\n",
    "    def __init__(self):\n",
    "        self.x_dim = 64 * 64 * 3\n",
    "        self.name = 'lsun/dcgan/d_net'\n",
    "\n",
    "    def __call__(self, x, target, reuse=True):\n",
    "        with tf.variable_scope(self.name) as vs:\n",
    "            if reuse:\n",
    "                vs.reuse_variables()\n",
    "            bs = tf.shape(x)[0]\n",
    "            x = tf.reshape(x, [bs, 64, 64, 3])\n",
    "            target = tf.reshape(target, [bs, 64, 64, 3])\n",
    "            x = tf.concat([x, target], axis=3)\n",
    "            conv1 = tcl.conv2d(\n",
    "                x, 64, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu\n",
    "            )\n",
    "            conv2 = tcl.conv2d(\n",
    "                conv1, 128, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu_batch_norm\n",
    "            )\n",
    "            conv3 = tcl.conv2d(\n",
    "                conv2, 256, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu_batch_norm\n",
    "            )\n",
    "            conv4 = tcl.conv2d(\n",
    "                conv3, 512, [4, 4], [2, 2],\n",
    "                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation_fn=leaky_relu_batch_norm\n",
    "            )\n",
    "            conv4 = tcl.flatten(conv4)\n",
    "            fc = tcl.fully_connected(conv4, 1, activation_fn=tf.identity)\n",
    "            return fc\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return [var for var in tf.global_variables() if self.name in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1xjLRo8S5TM"
   },
   "outputs": [],
   "source": [
    "class AdversarialGraph(object):\n",
    "  def __init__(self, max_seq_len=8, painter_type=\"VAE\", connected=True, gpu_mode=True, graph=None):\n",
    "    self.painter_type=painter_type\n",
    "    self.connected = connected\n",
    "    self.action_generator = ActionGenerator2()\n",
    "    self.d_net = DiscriminatorConditional()\n",
    "    self.data_dispenser = CelebADispenser()\n",
    "    self.max_seq_len = max_seq_len\n",
    "    \n",
    "    self.gpu_mode = gpu_mode\n",
    "    if not gpu_mode:\n",
    "      with tf.device('/cpu:0'):\n",
    "        tf.logging.info('Model using cpu.')\n",
    "        self._build_graph(graph)\n",
    "    else:\n",
    "      tf.logging.info('Model using gpu.')\n",
    "      self._build_graph(graph)\n",
    "    self._init_session()\n",
    "  \n",
    "  def get_random_batch(self, batch_size, train=True):\n",
    "    random_targets = self.data_dispenser.get_random_target(batch_size, train=train)\n",
    "    random_targets = random_targets.astype(np.float)/255.\n",
    "    return random_targets\n",
    "    \n",
    "    # sanity check\n",
    "    #self.target_np = self.mnist_env.real_data[1].reshape([1, 64, 64, 1])\n",
    "    #self.target_np_batch = np.tile(self.target_np, [batch_size,1,1,3]).astype(np.float)/255.0\n",
    "    #return self.target_np_batch\n",
    "  \n",
    "  def _build_graph(self, graph):\n",
    "    if graph is None:\n",
    "      self.g = tf.Graph()\n",
    "    else:\n",
    "      self.g = graph\n",
    "    \n",
    "    # Set up graphs of GAN\n",
    "    if self.painter_type == \"VAE\":\n",
    "      self.painter = ConvVAE(z_size=64, batch_size=100, \n",
    "                         learning_rate=1e-4, kl_tolerance=0.5, \n",
    "                         gpu_mode=self.gpu_mode, is_training=False, \n",
    "                         reuse=False, graph=self.g)\n",
    "    else:\n",
    "      self.painter = ConvGAN(learning_rate=1e-4,\n",
    "              is_training=False,\n",
    "              reuse=False,\n",
    "              gpu_mode=self.gpu_mode,\n",
    "              divisor=4,\n",
    "              add_noise=False,\n",
    "              graph=self.g)\n",
    "    self.painter.close_sess()\n",
    "    with self.g.as_default():\n",
    "      print(tf.global_variables())\n",
    "    \n",
    "    with self.g.as_default():\n",
    "      self.target_image = tf.placeholder(dtype=tf.float32, shape=[None, 64, 64, 3])\n",
    "      \n",
    "      batch_size = tf.shape(self.target_image)[0]\n",
    "            \n",
    "      # Prepare loop vars for rnn loop\n",
    "      canvas_state = tf.ones(shape=[batch_size, 64, 64, 3], dtype=tf.float32)\n",
    "      actions = tf.zeros(shape=[batch_size, 2], dtype=tf.float32)\n",
    "      action_cell_state, action_hidden_state = self.action_generator.create_cell().zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "      i = tf.constant(0)\n",
    "      initial_ta = tf.TensorArray(dtype=tf.float32, size=self.max_seq_len)\n",
    "      initial_canvas_ta = tf.TensorArray(dtype=tf.float32, size=self.max_seq_len)\n",
    "      loop_vars = (\n",
    "          canvas_state, actions, action_cell_state, action_hidden_state, \n",
    "          initial_ta, initial_canvas_ta, i)\n",
    "      \n",
    "      # Just make the graph variables so we can use it in the loop. TODO: Can't I create this in the loop itself?\n",
    "      self.action_generator(action_cell_state, action_hidden_state, actions, canvas_state, canvas_state, reuse=False)\n",
    "      \n",
    "      # condition for continuation\n",
    "      def cond(cs, ac, acs, ahs, i_ta, c_ta, i):\n",
    "        return tf.less(i, self.max_seq_len)\n",
    "      \n",
    "      # run one state of rnn cell\n",
    "      def body(cs, ac, acs, ahs, i_ta, c_ta, i):\n",
    "        i = tf.add(i, 1)\n",
    "        \n",
    "        prev_ac = ac\n",
    "        ac, (acs, ahs) = self.action_generator(acs, ahs, ac, cs, self.target_image)\n",
    "        i_ta = i_ta.write(i-1, ac)\n",
    "\n",
    "        def use_whole_action():\n",
    "          return ac\n",
    "        \n",
    "        def use_previous_entrypoint():\n",
    "          # start x and y are previous end x and y\n",
    "          # start pressure is previous pressure\n",
    "          return tf.concat([ac[:, :9], prev_ac[:, 4:6], prev_ac[:, 0:1]], axis=1)\n",
    "        \n",
    "        if self.connected:\n",
    "          ac = tf.cond(tf.equal(i, 1), true_fn=use_whole_action, false_fn=use_previous_entrypoint)\n",
    "        else:\n",
    "          ac = use_whole_action()\n",
    "        \n",
    "        decoded_stroke = self.painter.generate_stroke_graph(ac)\n",
    "\n",
    "        darkness_mask = tf.reduce_mean(decoded_stroke, axis=3)\n",
    "        darkness_mask = 1 - tf.reshape(darkness_mask, [batch_size, 64, 64, 1])\n",
    "        darkness_mask = darkness_mask / tf.reduce_max(darkness_mask)\n",
    "        \n",
    "        # TODO ISSUES HERE: rewrite logic for our setup with no MyPaint and Color...\n",
    "#         color_action = ac[:, 6:9]\n",
    "#         color_action = tf.reshape(color_action, [batch_size, 1, 1, 3])\n",
    "#         color_action = tf.tile(color_action, [1, 64, 64, 1])\n",
    "#         stroke_whitespace = tf.equal(decoded_stroke, 1.)\n",
    "#         maxed_stroke = tf.where(stroke_whitespace, decoded_stroke, color_action)\n",
    "        \n",
    "        cs = (darkness_mask)*maxed_stroke + (1-darkness_mask)*cs\n",
    "        c_ta = c_ta.write(i-1, cs)\n",
    "        \n",
    "        return (cs, ac, acs, ahs, i_ta, c_ta, i)\n",
    "      \n",
    "      final_canvas_state, ac, _, _, final_ta, final_canvas_ta, _ = tf.while_loop(cond, body, loop_vars, swap_memory=True)\n",
    "      self.final_canvas_state = final_canvas_state\n",
    "      self.last_actions = final_ta.stack()\n",
    "      self.intermediate_canvases = final_canvas_ta.stack()\n",
    "      \n",
    "      real_score = tf.reduce_mean(self.d_net(self.target_image, self.target_image, reuse=False))\n",
    "      generated_score = tf.reduce_mean(self.d_net(final_canvas_state, self.target_image))\n",
    "      \n",
    "      real_score_summ = tf.summary.scalar('real score', real_score)\n",
    "      g_loss_summ = tf.summary.scalar('generated score/g_loss', generated_score)\n",
    "      \n",
    "      self.g_loss = generated_score\n",
    "      reconstruction_loss = 10*tf.reduce_mean(tf.square(final_canvas_state-self.target_image))\n",
    "      recon_loss_summ = tf.summary.scalar('recon_loss', reconstruction_loss)\n",
    "      self.g_loss = self.g_loss + reconstruction_loss\n",
    "     \n",
    "      self.d_loss = real_score - generated_score\n",
    "      \n",
    "      real_min_gen_summ = tf.summary.scalar('real - generated', self.d_loss)\n",
    "      \n",
    "      epsilon = tf.random_uniform([], 0.0, 1.0)\n",
    "      x_hat = epsilon * self.target_image + (1 - epsilon) * final_canvas_state\n",
    "      d_hat = self.d_net(x_hat, self.target_image)\n",
    "      \n",
    "      # TODO: This might be wrong. might need to calc gradients wrt xhat concated with target\n",
    "      ddx = tf.gradients(d_hat, x_hat)[0]\n",
    "      print(ddx.get_shape().as_list())\n",
    "      #ddx = tf.sqrt(tf.reduce_sum(tf.square(ddx), axis=1))\n",
    "      ddx = tf.sqrt(tf.reduce_sum(tf.square(ddx), axis=(1, 2, 3)))\n",
    "      ddx = tf.reduce_mean(tf.square(ddx - 1.0) * 10.0)\n",
    "      \n",
    "      self.d_loss = self.d_loss + ddx\n",
    "      \n",
    "      gradient_penalty_summ = tf.summary.scalar('gradient penalty', ddx)\n",
    "      d_loss_summ = tf.summary.scalar('actual loss', self.d_loss)\n",
    "        \n",
    "      self.d_adam, self.g_adam = None, None\n",
    "      with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "          self.d_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9)\\\n",
    "              .minimize(self.d_loss, var_list=self.d_net.vars)\n",
    "          self.g_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9)\\\n",
    "              .minimize(self.g_loss, var_list=self.action_generator.vars)\n",
    "        \n",
    "\n",
    "      # initialize vars\n",
    "      self.init = tf.global_variables_initializer()\n",
    "      \n",
    "      #summary ops\n",
    "      real_im_summ = tf.summary.image('real_images', self.target_image, max_outputs=3)\n",
    "      gen_im_summ = tf.summary.image('generated_images', final_canvas_state, max_outputs=3)\n",
    "      self.summary_op = tf.summary.merge([real_im_summ, gen_im_summ, d_loss_summ, gradient_penalty_summ, \n",
    "                                          real_min_gen_summ, g_loss_summ, real_score_summ,\n",
    "                                          recon_loss_summ,\n",
    "                                         ])\n",
    "      \n",
    "  def train(self, batch_size=64, num_batches=1000000, last_actions_arr=[0, 1, 2], start_batch=0):\n",
    "    train_writer = tf.summary.FileWriter('logdir', self.g)\n",
    "    start_time = time.time()\n",
    "    for t in range(start_batch, num_batches):\n",
    "        d_iters = 5\n",
    "\n",
    "        for _ in range(0, d_iters):\n",
    "            _, summ = self.sess.run((self.d_adam, self.summary_op), feed_dict={self.target_image: self.get_random_batch(batch_size)})\n",
    "\n",
    "        rand_batch = self.get_random_batch(batch_size)\n",
    "        _, last_actions, fcstate, summ = self.sess.run((self.g_adam, self.last_actions, self.final_canvas_state, self.summary_op), feed_dict={self.target_image: rand_batch})\n",
    "        last_actions_arr[0] = last_actions\n",
    "        last_actions_arr[1] = fcstate\n",
    "        last_actions_arr[2] = rand_batch\n",
    "        train_writer.add_summary(summ, t)\n",
    "\n",
    "        if t % 100 == 0:\n",
    "\n",
    "            d_loss = self.sess.run(\n",
    "                self.d_loss, feed_dict={self.target_image: self.get_random_batch(batch_size)}\n",
    "            )\n",
    "            g_loss = self.sess.run(\n",
    "                self.g_loss, feed_dict={self.target_image: self.get_random_batch(batch_size)}\n",
    "            )\n",
    "            print('Iter [%8d] Time [%5.4f] d_loss [%.4f] g_loss [%.4f]' %\n",
    "                    (t, time.time() - start_time, d_loss, g_loss))\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            self.save_model('adversarial_global_vars_celeba')\n",
    "\n",
    "  def _init_session(self):\n",
    "    self.sess = tf.Session(graph=self.g)\n",
    "    self.sess.run(self.init)\n",
    "  def close_sess(self):\n",
    "    self.sess.close()\n",
    "    \n",
    "  def load_painter_checkpoint(self, checkpoint_path='tf_conv_vae', actual_path=None):\n",
    "    sess = self.sess\n",
    "    with self.g.as_default():\n",
    "      if self.painter_type == \"VAE\":\n",
    "        pth = 'conv_vae'\n",
    "      elif self.painter_type == \"GAN\":\n",
    "        pth = 'conv_gan'\n",
    "      saver = tf.train.Saver(tf.global_variables(pth))\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "    if actual_path is None:\n",
    "      actual_path = ckpt.model_checkpoint_path\n",
    "    print('loading model', actual_path)\n",
    "    tf.logging.info('Loading model %s.', actual_path)\n",
    "    saver.restore(sess, actual_path)\n",
    "\n",
    "  def save_model(self, model_save_path='adversarial_global_vars_celeba'):\n",
    "    sess = self.sess\n",
    "    with self.g.as_default():\n",
    "      saver = tf.train.Saver(tf.global_variables())\n",
    "    checkpoint_path = os.path.join(model_save_path, 'graph')\n",
    "    tf.logging.info('saving model %s.', checkpoint_path)\n",
    "    saver.save(sess, checkpoint_path, 0) # just keep one\n",
    "  def load_checkpoint(self, checkpoint_path='adversarial_global_vars_celeba'):\n",
    "    sess = self.sess\n",
    "    with self.g.as_default():\n",
    "      saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "    print('loading model', ckpt.model_checkpoint_path)\n",
    "    tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inDRcRgQYF_e"
   },
   "source": [
    "# Set up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "49mnMb4bWF2N",
    "outputId": "71f967f0-109f-4bb9-90d0-4aeb7be71a41"
   },
   "outputs": [],
   "source": [
    "# #!rm -r logdir\n",
    "\n",
    "\n",
    "# get_ipython().system_raw(\n",
    "#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "#     .format('logdir')\n",
    "# )\n",
    "\n",
    "# get_ipython().system_raw('./ngrok http 6006 &')\n",
    "\n",
    "# !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYqPzjOA029A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iv8bNS4BYInS"
   },
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm data/celeba/celeba_dict.pkl # TODO: for some reasons pickle doesn't work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "G518ssqYLh8D",
    "outputId": "6b2ccaeb-b801-41f4-ebe6-72d9ef7a50b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-03 14:57:29 \u001b[0m\u001b[1;31mSkip making directories: data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the graph. This step will also preprocess all CelebA images to 64x64 the first time you run it.\n",
    "lol = AdversarialGraph(max_seq_len=15, connected=False, painter_type='GAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "SWEM3sAHYTaq",
    "outputId": "65ee0aa5-4834-4224-e225-041418f7d7c9"
   },
   "outputs": [],
   "source": [
    "# Load GAN painter checkpoint\n",
    "lol.load_painter_checkpoint('tf_gan3')\n",
    "\n",
    "# Load VAE painter checkpoint\n",
    "# lol.load_painter_checkpoint('tf_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zi5PVNkjT4Ko"
   },
   "outputs": [],
   "source": [
    "# Resume training from checkpoint\n",
    "#lol.load_checkpoint('adversarial_global_vars_celeba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvR_By58YTWb"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 0 values, but the requested shape has 192\n\t [[node while/Reshape_1 (defined at <ipython-input-26-1debcfe5d13d>:105) ]]\n\t [[node mul (defined at <ipython-input-26-1debcfe5d13d>:127) ]]\n\nCaused by op 'while/Reshape_1', defined at:\n  File \"/home/dl/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/dl/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/dl/anaconda3/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/dl/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/dl/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-ace40896921e>\", line 2, in <module>\n    lol = AdversarialGraph(max_seq_len=15, connected=False, painter_type='GAN')\n  File \"<ipython-input-26-1debcfe5d13d>\", line 17, in __init__\n    self._build_graph(graph)\n  File \"<ipython-input-26-1debcfe5d13d>\", line 115, in _build_graph\n    final_canvas_state, ac, _, _, final_ta, final_canvas_ta, _ = tf.while_loop(cond, body, loop_vars, swap_memory=True)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3556, in while_loop\n    return_same_structure)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3087, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3022, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"<ipython-input-26-1debcfe5d13d>\", line 105, in body\n    color_action = tf.reshape(color_action, [batch_size, 1, 1, 3])\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7179, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 0 values, but the requested shape has 192\n\t [[node while/Reshape_1 (defined at <ipython-input-26-1debcfe5d13d>:105) ]]\n\t [[node mul (defined at <ipython-input-26-1debcfe5d13d>:127) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 0 values, but the requested shape has 192\n\t [[{{node while/Reshape_1}}]]\n\t [[{{node mul}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9cdd377e7c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlast_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_actions_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-1debcfe5d13d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, num_batches, last_actions_arr, start_batch)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mrand_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 0 values, but the requested shape has 192\n\t [[node while/Reshape_1 (defined at <ipython-input-26-1debcfe5d13d>:105) ]]\n\t [[node mul (defined at <ipython-input-26-1debcfe5d13d>:127) ]]\n\nCaused by op 'while/Reshape_1', defined at:\n  File \"/home/dl/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/dl/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/dl/anaconda3/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/dl/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/dl/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-ace40896921e>\", line 2, in <module>\n    lol = AdversarialGraph(max_seq_len=15, connected=False, painter_type='GAN')\n  File \"<ipython-input-26-1debcfe5d13d>\", line 17, in __init__\n    self._build_graph(graph)\n  File \"<ipython-input-26-1debcfe5d13d>\", line 115, in _build_graph\n    final_canvas_state, ac, _, _, final_ta, final_canvas_ta, _ = tf.while_loop(cond, body, loop_vars, swap_memory=True)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3556, in while_loop\n    return_same_structure)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3087, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3022, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"<ipython-input-26-1debcfe5d13d>\", line 105, in body\n    color_action = tf.reshape(color_action, [batch_size, 1, 1, 3])\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7179, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/dl/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 0 values, but the requested shape has 192\n\t [[node while/Reshape_1 (defined at <ipython-input-26-1debcfe5d13d>:105) ]]\n\t [[node mul (defined at <ipython-input-26-1debcfe5d13d>:127) ]]\n"
     ]
    }
   ],
   "source": [
    "last_arr = [None, None, None]\n",
    "lol.train(last_actions_arr = last_arr, start_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JuDRb9-4YTTR",
    "outputId": "821af323-ebe6-41f1-b6d6-846744c7a19f"
   },
   "outputs": [],
   "source": [
    "lol.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "il9R-sg9e_ui"
   },
   "source": [
    "# evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cYl4noTGgv-"
   },
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "  h=args.screen_size\n",
    "  fig=plt.figure(figsize=(16, 16))\n",
    "  columns = len(images)\n",
    "  rows = 1\n",
    "\n",
    "  for i, img in enumerate(images):\n",
    "    img = img[:, :, :3]\n",
    "    #print(img.shape)\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(img)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPsvuD2fGgeK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FhXOE7beodx"
   },
   "outputs": [],
   "source": [
    "_realEnv = ColorEnv(args, paint_mode=PaintMode.JUMP_STROKES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "colab_type": "code",
    "id": "NGE8ysZvhIoP",
    "outputId": "cc0e4049-98b4-46c7-cd12-a76392baa86c"
   },
   "outputs": [],
   "source": [
    "def run_actions_on_real_env(realEnv, actions_array):\n",
    "  images = []\n",
    "  realEnv.reset()\n",
    "  for ac in actions_array:\n",
    "    print(ac)\n",
    "    realEnv.draw(ac.astype(np.float64))\n",
    "    images.append(realEnv.image)\n",
    "  plot_images(images)\n",
    "run_actions_on_real_env(_realEnv, last_arr[0][:, 41, :])\n",
    "plot_images(last_arr[1][40:48])\n",
    "plot_images(last_arr[2][40:48])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdWQLpFGOVyA"
   },
   "source": [
    "# Generate animations with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5j5YmMWSHmP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwweoPMIOGXz"
   },
   "outputs": [],
   "source": [
    "_ran_batch = lol.get_random_batch(20, train=False)\n",
    "_last_actions, _fcstate, _int_canvases = lol.sess.run((lol.last_actions, lol.final_canvas_state, lol.intermediate_canvases), feed_dict={lol.target_image: _ran_batch})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mEDF6zMfOGV5",
    "outputId": "73bcefb3-8131-4236-b8bb-6b8f3b770aa2"
   },
   "outputs": [],
   "source": [
    "_fcstate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DnFZR29lOGUD",
    "outputId": "b1743a52-eadd-4bb8-b29f-967483b7ac7a"
   },
   "outputs": [],
   "source": [
    "_last_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kc7sr8ueSVOw",
    "outputId": "7b0fac6e-c79f-474a-edb8-d45fbef394de"
   },
   "outputs": [],
   "source": [
    "_int_canvases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "4aZ06qxwOGRQ",
    "outputId": "459a6c19-fe09-4242-943b-2b371e2b0ef5"
   },
   "outputs": [],
   "source": [
    "def run_actions_on_real_env(realEnv, actions_array):\n",
    "  images = []\n",
    "  realEnv.reset()\n",
    "  for ac in actions_array:\n",
    "    realEnv.draw(ac.astype(np.float64))\n",
    "    images.append(realEnv.image)\n",
    "  return images\n",
    "_imgs = run_actions_on_real_env(_realEnv, _last_actions[:, 9, :])\n",
    "plot_images(_imgs[80:])\n",
    "plot_images(_int_canvases[:, 9, :, :])\n",
    "plot_images(_fcstate[:10])\n",
    "plot_images(_ran_batch[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "jPdvrrV6UN_C",
    "outputId": "4f416a20-7dc3-42d4-d2a2-e160d3886f94"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "_final_to_plot = []\n",
    "for _target in range(20):\n",
    "  run_actions_on_real_env(_realEnv, _last_actions[:, _target, :])\n",
    "  print(len(_realEnv.intermediate_images))\n",
    "  _realEnv.intermediate_images = _realEnv.intermediate_images[0::30]\n",
    "\n",
    "  _inter_images = np.stack(_realEnv.intermediate_images)[:, :, :, :3].astype(np.uint8)#.astype(np.float)/255.\n",
    "  _target_images = (np.tile(_ran_batch[_target].reshape(1, 64, 64, 3), [len(_realEnv.intermediate_images), 1, 1, 1])*255).astype(np.uint8)\n",
    "\n",
    "  _plot = np.concatenate([_target_images, _inter_images], axis=2)\n",
    "  _final_to_plot.append(_plot)\n",
    "\n",
    "_rows = 5\n",
    "_cols = 4\n",
    "_rows_to_stack = []\n",
    "for i in range(_rows):\n",
    "  _rows_to_stack.append(np.concatenate(_final_to_plot[i*_cols:(i+1)*_cols], axis=2))\n",
    "\n",
    "\n",
    "clip = mpy.ImageSequenceClip([x for x in (np.concatenate(_rows_to_stack, axis=1))], fps=14)\n",
    "clip.write_videofile(\"hello.mp4\")\n",
    "display(mpy.ipython_display('hello.mp4', height=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1140
    },
    "colab_type": "code",
    "id": "6DvNirCyTWNs",
    "outputId": "df51da85-0ccd-48a9-da96-a709e5eb3d2f"
   },
   "outputs": [],
   "source": [
    "\n",
    "_stacked_plots = []\n",
    "for _target in range(5, 20):\n",
    "  run_actions_on_real_env(_realEnv, _last_actions[:, _target, :])\n",
    "  print(len(_realEnv.intermediate_images))\n",
    "  _realEnv.intermediate_images = _realEnv.intermediate_images[0::37]\n",
    "\n",
    "  _inter_images = np.stack(_realEnv.intermediate_images)[:, :, :, :3].astype(np.float)/255.\n",
    "  _intermediate_canvases_to_plot = np.repeat(_int_canvases[:, _target, :, :, :], 53, axis=0)[0::19]\n",
    "  _target_images = np.tile(_ran_batch[_target].reshape(1, 64, 64, 3), [len(_realEnv.intermediate_images), 1, 1, 1])\n",
    "\n",
    "  print(len(_realEnv.intermediate_images))\n",
    "  print(_target_images.shape, _intermediate_canvases_to_plot.shape, _inter_images.shape)\n",
    "\n",
    "  _plot = np.concatenate([_target_images, _intermediate_canvases_to_plot, _inter_images], axis=2)\n",
    "  _stacked_plots.append(_plot)\n",
    "\n",
    "  \n",
    "clip = mpy.ImageSequenceClip([x for x in (np.concatenate(_stacked_plots)*255).astype(np.uint8)], fps=14)\n",
    "clip.write_videofile(\"hello2.mp4\")\n",
    "display(mpy.ipython_display('hello2.mp4', height=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7sJq-2sRu2U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWwcVEqwRuy0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zs3bfiUQQG2_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cwY41zbQG0j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAWjrMDqQGx2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04T_2GXfuY1o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "recreating-spiral.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:paint]",
   "language": "python",
   "name": "conda-env-paint-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
